# Serverless Data Processing with Dataflow - Writing an ETL Pipeline using Apache Beam and Dataflow
## Overview

1. In this lab, you will learn how to:
    - Build a batch Extract-Transform-Load pipeline in Apache Beam, which takes raw data from Google Cloud Storage and writes it to BigQuery.
Run the Apache Beam pipeline on Dataflow.
Parameterize the execution of the pipeline.

## Prerequisites:

1. Basic familiarity with Python.

## Jupyter notebook-based development environment setup
1. For this lab, you will be running all commands in a terminal from your notebook.
2. In the Google Cloud Console, on the Navigation Menu, click Vertex AI > Workbench.
3. Click Enable Notebooks API.
4. On the Workbench page, select USER-MANAGED NOTEBOOKS and click CREATE NEW.
5. In the New instance dialog box that appears, set the region to region and zone to zone.
6. For Environment, select Apache Beam.
7. Click CREATE at the bottom of the dialog vox.
8. Note: The environment may take 3 - 5 minutes to be fully provisioned. Please wait until the step is complete.
9. Note: Click Enable Notebook API to enable the notebook api.
10. Once the environment is ready, click the OPEN JUPYTERLAB link next to your Notebook name. This will open up your environment in a new tab in your browser.
11. Next, click Terminal. This will open up a terminal where you can run all the commands in this lab.
12. Open Terminal

## Download Code Repository
1. Next you will download a code repository for use in this lab.
2. In the terminal you just opened, enter the following:
``` git
git clone https://github.com/GoogleCloudPlatform/training-data-analyst
cd /home/jupyter/training-data-analyst/quests/dataflow_python/
```
3. On the left panel of your notebook environment, in the file browser, you will notice the training-data-analyst repo added.
4. Navigate into the cloned repo /training-data-analyst/quests/dataflow_python/. You will see a folder for each lab, which is further divided into a lab sub-folder with code to be completed by you, and a solution sub-folder with a fully workable example to reference if you get stuck.
5. Note: To open a file for editing purposes, simply navigate to the file and click on it. This will open the file, where you can add or modify code.

## Apache Beam and Dataflow

1. Dataflow is a fully-managed Google Cloud service for running batch and streaming Apache Beam data processing pipelines.
2. Apache Beam is an open source, advanced, unified, and portable data processing programming model that allows end users to define both batch and streaming data-parallel processing pipelines using Java, Python, or Go. Apache Beam pipelines can be executed on your local development machine on small datasets, and at scale on Dataflow. However, because Apache Beam is open source, other runners exist — you can run Beam pipelines on Apache Flink and Apache Spark, among others.

![image](https://github.com/user-attachments/assets/6c456b95-0255-49ce-a86a-0eba3a4db7f7)

## Lab part 1. Writing an ETL pipeline from scratch

### Introduction

1. In this section, you write an Apache Beam Extract-Transform-Load (ETL) pipeline from scratch.
2. Dataset and use case review
3. For each lab in this quest, the input data is intended to resemble web server logs in Common Log format along with other data that a web server might contain. For this first lab, the data is treated as a batch source; in later labs, the data will be treated as a streaming source. Your task is to read the data, parse it, and then write it to BigQuery, a serverless data warehouse, for later data analysis.
4. Open the appropriate lab
5. Return to the terminal in your IDE, and copy and paste the following command:
``` sh
cd 1_Basic_ETL/solution
export BASE_DIR=$(pwd)
```
6. Set up the virtual environment and dependencies
7. Before you can begin editing the actual pipeline code, you need to ensure that you have installed the necessary dependencies.
8. In the terminal, create a virtual environment for your work in this lab:
``` sh
sudo apt-get update && sudo apt-get install -y python3-venv
```
``` py
python3 -m venv df-env
```
``` py
source df-env/bin/activate
```
9. Next, install the packages you will need to execute your pipeline:
``` py
python3 -m pip install -q --upgrade pip setuptools wheel
python3 -m pip install apache-beam[gcp]
```
10. Finally, ensure that the Dataflow API is enabled:
``` sh
gcloud services enable dataflow.googleapis.com
```
# Write your first pipeline

## Task 1. Generate synthetic data

1. Run the following command in the terminal to clone a repository containing scripts for generating synthetic web server logs:
``` sh
cd $BASE_DIR/../..
source create_batch_sinks.sh
bash generate_batch_events.sh
head events.json
```
2. The script creates a file called events.json containing lines resembling the following:
``` json
{"user_id": "-6434255326544341291", "ip": "192.175.49.116", "timestamp": "2019-06-19T16:06:45.118306Z", "http_request": "\"GET eucharya.html HTTP/1.0\"", "lat": 37.751, "lng": -97.822, "http_response": 200, "user_agent": "Mozilla/5.0 (compatible; MSIE 7.0; Windows NT 5.01; Trident/5.1)", "num_bytes": 182}
```
3. It then automatically copies this file to your Google Cloud Storage bucket at Cloud Storage input path.
4. In another browser tab, navigate to Google Cloud Storage and confirm that your storage bucket contains a file called events.json.
5. Click Check my progress to verify the objective.
6. Generate synthetic data


## Task 2. Read data from your source

1. Understand the code of labdata/my_pipeline.py

## Task 3. Run your pipeline

1. Return to the terminal, and run your pipeline.

``` sh
# Set up environment variables
cd $BASE_DIR
export PROJECT_ID=$(gcloud config get-value project)

# Run the pipelines
python3 my_pipeline.py \
  --project=${PROJECT_ID} \
  --region=Region \
  --stagingLocation=gs://$PROJECT_ID/staging/ \
  --tempLocation=gs://$PROJECT_ID/temp/ \
  --runner=DataflowRunner
```
2. The overall shape should be a single path, starting with the Read transform and ending with the Write transform. As your pipeline runs, workers will be added automatically, as the service determines the needs of your pipeline, and then disappear when they are no longer needed. You can observe this by navigating to Compute Engine, where you should see virtual machines created by the Dataflow service.
3. Note: If your pipeline is building successfully, but you're seeing a lot of errors due to code or misconfiguration in the Dataflow service, you can set runner back to DirectRunner to run it locally and receive faster feedback. This approach works in this case because the dataset is small and you are not using any features that aren't supported by DirectRunner.
4. Once your pipeline has finished, return to the BigQuery browser window and query your table.



# Lab part 2. Parameterizing basic ETL


1. Much of the work of data engineers is either predictable, like recurring jobs, or it’s similar to other work. However, the process for running pipelines requires engineering expertise. Think back to the steps that you just completed:
2. You created a development environment and developed a pipeline. The environment included the Apache Beam SDK and other dependencies.
3. You executed the pipeline from the development environment. The Apache Beam SDK staged files in Cloud Storage, created a job request file, and submitted the file to the Dataflow service.
4. It would be much better if there were a way to initiate a job through an API call or without having to set up a development environment (which non-technical users would be unable to do). This would also allow you to run pipelines.
5. Dataflow Templates seek to solve this problem by changing the representation that is created when a pipeline is compiled so that it is parameterizable. Unfortunately, it is not as simple as exposing command-line parameters, although that is something you do in a later lab. With Dataflow Templates, the workflow above becomes:
6. Developers create a development environment and develop their pipeline. The environment includes the Apache Beam SDK and other dependencies.
7. Developers execute the pipeline and create a template. The Apache Beam SDK stages files in Cloud Storage, creates a template file (similar to job request), and saves the template file in Cloud Storage.
8. Non-developer users or other workflow tools like Airflow can easily execute jobs with the Google Cloud console, gcloud command-line tool, or the REST API to submit template file execution requests to the Dataflow service.
9. In this lab, you will practice using one of the many Google-created Dataflow Templates to accomplish the same task as the pipeline that you built in Part 1.

## Task 1. Create a JSON schema file

1. Just like before, you must pass the Dataflow Template a JSON file representing the schema in this example.
2. Return to the terminal in your IDE. Run the following commands to navigate back to the main directory, then grab the schema from your existing logs.logs table:
``` sh
cd $BASE_DIR/../..
bq show --schema --format=prettyjson logs.logs
```
3. Now, capture this output in a file and upload to GCS. The extra sed commands are to build a full JSON object that Dataflow will expect.
``` sh
bq show --schema --format=prettyjson logs.logs | sed '1s/^/{"BigQuery Schema":/' | sed '$s/$/}/' > schema.json
cat schema.json


export PROJECT_ID=$(gcloud config get-value project)
gcloud storage cp schema.json gs://${PROJECT_ID}/
```
## Task 2. Write a JavaScript user-defined function
1. The Cloud Storage to BigQuery Dataflow Template requires a JavaScript function to convert the raw text into valid JSON. In this case, each line of text is valid JSON, so the function is somewhat trivial.

2. To complete this task, create a New File in the dataflow_python folder in the file explorer of your IDE.

3. To create New File, click on File >> New >> Text File.

4. Rename the file name as transform.js, to rename the file name right click on it.

5. Open transform.js file in the editor panel, click on the file to open it.

6. Copy the function below to the transform.js file and save it:

``` java
function transform(line) {
  return line;
}
```
7. Then run the following to copy the file to Google Cloud Storage:
``` sh
export PROJECT_ID=$(gcloud config get-value project)
gcloud storage cp *.js gs://${PROJECT_ID}/
```

## Task 3. Run a Dataflow Template

1. Go to the Dataflow Web UI.
2. Click CREATE JOB FROM TEMPLATE.
3. Enter a Job name for your Dataflow job.
4. Under Dataflow template, select the Text Files on Cloud Storage to BigQuery template under the Process Data in Bulk (batch) section, NOT the Streaming section.
5. Under Cloud Storage Input File, enter the path to events.json in the form Cloud Storage input path
6. Under Cloud Storage location of your BigQuery schema file, write the path to your schema.json file, in the form JSON path
7. Under BigQuery output table, enter BigQuery output table
8. Under Temporary BigQuery directory, enter a new folder within this same bucket. The job will create it for you.
9. Under Temporary location, enter a second new folder within this same bucket.
10. Leave Encryption at Google-managed encryption key.
11. Click to open Optional Prameters.
12. Under JavaScript UDF path in Cloud Storage, enter in the path to your .js, in the form JavaScript UDF path
13. Under JavaScript UDF name, enter transform.
14. Click the Run job button.

## Task 4. Inspect the Dataflow Template code

1. The code for the Dataflow Template you just used is located in this TextIOToBigQuery guide.
2. Scroll down to the main method. The code should look familiar to the pipeline you authored!
3. It begins with a Pipeline object, created using a PipelineOptions object.
4. It consists of a chain of PTransforms, beginning with a TextIO.read() transform.
5. The PTransform after the read transform is a bit different; it allows one to use Javascript to transform the input strings if, for example, the source format doesn’t align well with the BigQuery table format; for documentation on how to use this feature, see this page.
6. The PTransform after the Javascript UDF uses a library function to convert the Json into a tablerow; you can inspect that code here.
7. The write PTransform looks a bit different because instead of making use of a schema that is known at graph compile-time, the code is intended to accept parameters that will only be known at run-time. The NestedValueProvider class is what makes this possible.
8. Make sure to check out the next lab, which will cover making pipelines that are not simply chains of PTransforms, and how you can adapt a pipeline you’ve built to be a custom Dataflow Template.








